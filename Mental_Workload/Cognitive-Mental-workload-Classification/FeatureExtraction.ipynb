{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pywt\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "from spectrum import *\n",
    "from os import listdir\n",
    "from nitime import utils\n",
    "import scipy.stats as sp\n",
    "from os.path import isfile, join\n",
    "from nitime.viz import plot_tseries\n",
    "from matplotlib import pyplot as plt\n",
    "from nitime import algorithms as alg\n",
    "from nitime.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features List!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# names = ['Fractal Dimension','Coeffiecient of Variation','Mean of Vertex to Vertex Slope','Variance of Vertex to Vertex Slope',\n",
    "#          'Hjorth_Activity','Hjorth_Mobility','Hjorth_Complexity',\n",
    "#          'Kurtosis','2nd Difference Mean','2nd Difference Max',\n",
    "#          'Skewness','1st Difference Mean','1st Difference Max',\n",
    "#          'FFT Delta MaxPower','FFT Theta MaxPower','FFT Alpha MaxPower','FFT Beta MaxPower','Delta/Theta','Delta/Alpha','Theta/Alpha','(Delta+Theta)/Alpha',\n",
    "#          '1Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '2Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '3Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '4Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '5Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '6Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '7Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '8Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '9Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '10Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '11Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '12Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '13Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          '14Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "#          'AR1','AR2','AR3','AR4','AR5','AR6','AR7','AR8','AR9','AR10','AR11','AR12','AR13','AR14','AR15','AR16','AR17','AR18',\n",
    "#          'AR19','AR20','AR21','AR22','AR23','AR24','AR25','AR26','AR27','AR28','AR29','AR30','AR31','AR32','AR33','AR34','AR35','AR36','AR37','AR38','AR39','AR40','AR41','AR42'\n",
    "#         ]\n",
    "names = ['Coeffiecient of Variation','Mean of Vertex to Vertex Slope','Variance of Vertex to Vertex Slope',\n",
    "         'Hjorth_Activity','Hjorth_Mobility','Hjorth_Complexity',\n",
    "         'Kurtosis','2nd Difference Mean','2nd Difference Max',\n",
    "         'Skewness','1st Difference Mean','1st Difference Max',\n",
    "         'FFT Delta MaxPower','FFT Theta MaxPower','FFT Alpha MaxPower','FFT Beta MaxPower','Delta/Theta','Delta/Alpha','Theta/Alpha','(Delta+Theta)/Alpha',\n",
    "         '1Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '2Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '3Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '4Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '5Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '6Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '7Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '8Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '9Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '10Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '11Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '12Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '13Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '14Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         'AR1','AR2','AR3','AR4','AR5','AR6','AR7','AR8','AR9','AR10','AR11','AR12','AR13','AR14','AR15','AR16','AR17','AR18',\n",
    "         'AR19','AR20','AR21','AR22','AR23','AR24','AR25','AR26','AR27','AR28','AR29','AR30','AR31','AR32','AR33','AR34','AR35','AR36','AR37','AR38','AR39','AR40','AR41','AR42'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractal Dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fractal_dimension(Z, threshold=0.9):\n",
    "\n",
    "    # Only for 2d image\n",
    "    assert(len(Z.shape) == 2)\n",
    "\n",
    "    # From https://github.com/rougier/numpy-100 (#87)\n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "\n",
    "        # We count non-empty (0) and non-full boxes (k*k)\n",
    "        return len(np.where((S > 0) & (S < k*k))[0])\n",
    "\n",
    "\n",
    "    # Transform Z into a binary array\n",
    "    Z = (Z < threshold)\n",
    "\n",
    "    # Minimal dimension of image\n",
    "    p = min(Z.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "\n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    return -coeffs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Varaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coeff_var(a):\n",
    "    b = a #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        mean_i = np.mean(i) #Saving the mean of array i\n",
    "        std_i = np.std(i) #Saving the standard deviation of array i\n",
    "        output[k] = std_i/mean_i #computing coefficient of variation\n",
    "        k=k+1\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of Vertex to Vertex Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "def first_diff(i):\n",
    "    b=i    \n",
    "    \n",
    "    out = np.zeros(len(b))\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
    "        \n",
    "        j=j+1\n",
    "        c=out[1:len(out)]\n",
    "    return c #returns first diff\n",
    "\n",
    "\n",
    "def slope_mean(p):\n",
    "    b = p #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    res = np.zeros(len(b)-1)\n",
    "    \n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        x=i\n",
    "        amp_max = i[argrelextrema(x, np.greater)[0]]\n",
    "        t_max = argrelextrema(x, np.greater)[0]\n",
    "        amp_min = i[argrelextrema(x, np.less)[0]]\n",
    "        t_min = argrelextrema(x, np.less)[0]\n",
    "        t = np.concatenate((t_max,t_min),axis=0)\n",
    "        t.sort()#sort on the basis of time\n",
    "\n",
    "        h=0\n",
    "        amp = np.zeros(len(t))\n",
    "        res = np.zeros(len(t)-1)\n",
    "        for l in range(len(t)):\n",
    "            amp[l]=i[t[l]]\n",
    "           \n",
    "        \n",
    "        amp_diff = first_diff(amp)\n",
    "        \n",
    "        t_diff = first_diff(t)\n",
    "        \n",
    "        for q in range(len(amp_diff)):\n",
    "            res[q] = amp_diff[q]/t_diff[q]         \n",
    "        output[k] = np.mean(res) \n",
    "        k=k+1\n",
    "    return np.sum(output)/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of Vertex to Vertex Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "def first_diff(i):\n",
    "    b=i    \n",
    "    \n",
    "    out = np.zeros(len(b))\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
    "        \n",
    "        j=j+1\n",
    "        c=out[1:len(out)]\n",
    "    return c #returns first diff\n",
    "\n",
    "\n",
    "def slope_var(p):\n",
    "    b = p #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    res = np.zeros(len(b)-1)\n",
    "    \n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        x=i\n",
    "        amp_max = i[argrelextrema(x, np.greater)[0]]#storing maxima value\n",
    "        t_max = argrelextrema(x, np.greater)[0]#storing time for maxima\n",
    "        amp_min = i[argrelextrema(x, np.less)[0]]#storing minima value\n",
    "        t_min = argrelextrema(x, np.less)[0]#storing time for minima value\n",
    "        t = np.concatenate((t_max,t_min),axis=0) #making a single matrix of all matrix\n",
    "        t.sort() #sorting according to time\n",
    "\n",
    "        h=0\n",
    "        amp = np.zeros(len(t))\n",
    "        res = np.zeros(len(t)-1)\n",
    "        for l in range(len(t)):\n",
    "            amp[l]=i[t[l]]\n",
    "           \n",
    "        \n",
    "        amp_diff = first_diff(amp)\n",
    "        \n",
    "        t_diff = first_diff(t)\n",
    "        \n",
    "        for q in range(len(amp_diff)):\n",
    "            res[q] = amp_diff[q]/t_diff[q] #calculating slope        \n",
    "    \n",
    "        output[k] = np.var(res) \n",
    "        k=k+1#counting k\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hjorth    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hjorth(input):                                             # function for hjorth \n",
    "    realinput = input\n",
    "    hjorth_activity = np.zeros(len(realinput))\n",
    "    hjorth_mobility = np.zeros(len(realinput))\n",
    "    hjorth_diffmobility = np.zeros(len(realinput))\n",
    "    hjorth_complexity = np.zeros(len(realinput))\n",
    "    diff_input = np.diff(realinput)\n",
    "    diff_diffinput = np.diff(diff_input)\n",
    "    k = 0\n",
    "    for j in realinput:\n",
    "        hjorth_activity[k] = np.var(j)\n",
    "        hjorth_mobility[k] = np.sqrt(np.var(diff_input[k])/hjorth_activity[k])\n",
    "        hjorth_diffmobility[k] = np.sqrt(np.var(diff_diffinput[k])/np.var(diff_input[k]))\n",
    "        hjorth_complexity[k] = hjorth_diffmobility[k]/hjorth_mobility[k]\n",
    "        k = k+1\n",
    "    return np.sum(hjorth_activity)/14, np.sum(hjorth_mobility)/14, np.sum(hjorth_complexity)/14                       #returning hjorth activity, hjorth mobility , hjorth complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kurtosis(a):\n",
    "    b = a # Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    k = 0; # For counting the current row no.\n",
    "    for i in b:\n",
    "        mean_i = np.mean(i) # Saving the mean of array i\n",
    "        std_i = np.std(i) # Saving the standard deviation of array i\n",
    "        t = 0.0\n",
    "        for j in i:\n",
    "            t += (pow((j-mean_i)/std_i,4)-3)\n",
    "        kurtosis_i = t/len(i) # Formula: (1/N)*(summation(x_i-mean)/standard_deviation)^4-3\n",
    "        output[k] = kurtosis_i # Saving the kurtosis in the array created\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Difference Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def secDiffMean(a):\n",
    "    b = a # Extracting the data of the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
    "    k = 0; # For counting the current row no.\n",
    "    for i in b:\n",
    "        t = 0.0\n",
    "        for j in range(len(i)-1):\n",
    "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "        for j in range(len(i)-2):\n",
    "            t += abs(temp1[j+1]-temp1[j]) # Summing the 2nd Diffs\n",
    "        output[k] = t/(len(i)-2) # Calculating the mean of the 2nd Diffs\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Difference Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def secDiffMax(a):\n",
    "    b = a # Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
    "    k = 0; # For counting the current row no.\n",
    "    t = 0.0\n",
    "    for i in b:\n",
    "        for j in range(len(i)-1):\n",
    "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "        t = temp1[1] - temp1[0]\n",
    "        for j in range(len(i)-2):\n",
    "            if abs(temp1[j+1]-temp1[j]) > t :\n",
    "                t = temp1[j+1]-temp1[j] # Comparing current Diff with the last updated Diff Max\n",
    "\n",
    "        output[k] = t # Storing the 2nd Diff Max for channel k\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skewness(arr):\n",
    "    data = arr \n",
    "    skew_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        skew_array[index]=sp.stats.skew(i,axis=0,bias=True)\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(skew_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First Difference Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first_diff_mean(arr):\n",
    "    data = arr \n",
    "    diff_mean_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        sum=0.0#initializing the sum at the start of each iteration\n",
    "        for j in range(len(i)-1):\n",
    "            sum += abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "           \n",
    "        diff_mean_array[index]=sum/(len(i)-1)\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(diff_mean_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First Difference Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first_diff_max(arr):\n",
    "    data = arr \n",
    "    diff_max_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    first_diff = np.zeros(len(data[0])-1)#Initialinling the array as all 0s \n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        max=0.0#initializing at the start of each iteration\n",
    "        for j in range(len(i)-1):\n",
    "            first_diff[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "            if first_diff[j]>max: \n",
    "                max=first_diff[j] # finding the maximum of the first differences\n",
    "        diff_max_array[index]=max\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(diff_max_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Fetures!\n",
    "### Approx Mean, Approx Std Deviation, Approx Energy, Detailed Mean, Detailed Std Deviation, Detailed Energy, Approx Entropy & Detailed Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wavelet_features(epoch,channels):\n",
    "    cA_values = []\n",
    "    cD_values = []\n",
    "    cA_mean = []\n",
    "    cA_std = []\n",
    "    cA_Energy =[]\n",
    "    cD_mean = []\n",
    "    cD_std = []\n",
    "    cD_Energy = []\n",
    "    Entropy_D = []\n",
    "    Entropy_A = []\n",
    "    wfeatures = []\n",
    "    for i in range(channels):\n",
    "        cA,cD=pywt.dwt(epoch[i,:],'coif1')\n",
    "        cA_values.append(cA)\n",
    "        cD_values.append(cD)\t\t#calculating the coefficients of wavelet transform.\n",
    "    for x in range(channels):   \n",
    "        cA_mean.append(np.mean(cA_values[x]))\n",
    "        wfeatures.append(np.mean(cA_values[x]))\n",
    "        \n",
    "        cA_std.append(abs(np.std(cA_values[x])))\n",
    "        wfeatures.append(abs(np.std(cA_values[x])))\n",
    "        \n",
    "        cA_Energy.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        \n",
    "        cD_mean.append(np.mean(cD_values[x]))\t\t# mean and standard deviation values of coefficents of each channel is stored .\n",
    "        wfeatures.append(np.mean(cD_values[x]))\n",
    "\n",
    "        cD_std.append(abs(np.std(cD_values[x])))\n",
    "        wfeatures.append(abs(np.std(cD_values[x])))\n",
    "        \n",
    "        cD_Energy.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        \n",
    "        Entropy_D.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
    "        \n",
    "        Entropy_A.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x]))))) \n",
    "        wfeatures.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x])))))\n",
    "    return wfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT Max Power - Delta, Theta, Alpha & Beta Band!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def maxPwelch(data_win,Fs):\n",
    " \n",
    "    \n",
    "    BandF = [0.1, 3, 7, 12, 30]\n",
    "    PMax = np.zeros([14,(len(BandF)-1)]);\n",
    "    \n",
    "    for j in range(14):\n",
    "        f,Psd = signal.welch(data_win[j,:], Fs)\n",
    "        \n",
    "        for i in range(len(BandF)-1):\n",
    "            fr = np.where((f>BandF[i]) & (f<=BandF[i+1]))\n",
    "            PMax[j,i] = np.max(Psd[fr])\n",
    "    \n",
    "    return np.sum(PMax[:,0])/14,np.sum(PMax[:,1])/14,np.sum(PMax[:,2])/14,np.sum(PMax[:,3])/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanon Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shanon_entropy(labels): # Shanon Entropy\n",
    "    \"\"\" Computes entropy of 0-1 vector. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "    counts = np.bincount(labels)\n",
    "    probs = counts[np.nonzero(counts)] / n_labels\n",
    "    \n",
    "    n_classes = len(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "    return - np.sum(probs * np.log(probs)) / np.log(n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Spectral Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import fft\n",
    "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
    "from numpy.linalg import svd, lstsq\n",
    "import time\n",
    "\n",
    "def bin_power(X,Band,Fs):\n",
    "    C = fft(X)\n",
    "    C = abs(C)\n",
    "    Power =zeros(len(Band)-1);\n",
    "    for Freq_Index in xrange(0,len(Band)-1):\n",
    "        Freq = float(Band[Freq_Index])   ## Xin Liu\n",
    "        Next_Freq = float(Band[Freq_Index+1])\n",
    "        Power[Freq_Index] = sum(C[floor(Freq/Fs*len(X)):floor(Next_Freq/Fs*len(X))])\n",
    "    Power_Ratio = Power/sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "\n",
    "def spectral_entropy(X, Fs, Power_Ratio = None):\n",
    "    \n",
    "    Band = [0.1, 3, 7, 12, 30]\n",
    "    if Power_Ratio is None:\n",
    "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
    "\n",
    "    Spectral_Entropy = 0\n",
    "    for i in xrange(0, len(Power_Ratio) - 1):\n",
    "        Spectral_Entropy += Power_Ratio[i] * log(Power_Ratio[i])\n",
    "    Spectral_Entropy /= log(len(Power_Ratio))     # to save time, minus one is omitted\n",
    "    print('Shape of Spectral Entropy = ',n.shape(Spectral_Entropy))\n",
    "    return -1 * Spectral_Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression - Yule Walker Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autogressiveModelParameters(labels):\n",
    "    b_labels = len(labels)\n",
    "    feature = []\n",
    "    for i in range(14):\n",
    "        coeff, sig = alg.AR_est_YW(labels[i,:], 11,)\n",
    "        feature.append(coeff)\n",
    "    a = []     \n",
    "    for i in range(11):\n",
    "        a.append(np.sum(feature[:][i])/14)\n",
    "     \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression  - Burg Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autogressiveModelParametersBurg(labels):\n",
    "    feature = []\n",
    "    feature1 = []\n",
    "    model_order = 3\n",
    "    for i in range(14):\n",
    "        AR, rho, ref = arburg(labels[i], model_order)\n",
    "        feature.append(AR);\n",
    "    for j in range(14):\n",
    "        for i in range(model_order):\n",
    "            feature1.append(feature[j][i])\n",
    "\n",
    "    return feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S01-01-25.09.2016.10.45.01.edf']\n"
     ]
    }
   ],
   "source": [
    "DualTwoBackfiles = [f for f in listdir('Training-Data/S01/Dual-2-Back') if isfile(join('Training-Data/S01/Dual-2-Back', f))]\n",
    "print(DualTwoBackfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/Dual-2-Back/S01-01-25.09.2016.10.45.01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "<class 'numpy.ndarray'>\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: COUNTER, INTERPOLATED, AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, ...\n",
      " chs: 37 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 64.0 Hz\n",
      " meas_date: 2020-09-25 10:45:01 UTC\n",
      " nchan: 37\n",
      " projs: []\n",
      " sfreq: 128.0 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "path = 'Training-Data/S01/Dual-2-Back/S01-01-25.09.2016.10.45.01.edf'\n",
    "data = mne.io.read_raw_edf(path)\n",
    "# print(data_path)\n",
    "# print(data.get_data())\n",
    "print(type(data.get_data()))\n",
    "print(data.info)\n",
    "# print(data.eeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/Idle/S01-01-25.09.2016.10.52.46.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(24192, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7420/506323425.py:7: DeprecationWarning: Please use `skew` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  skew_array[index]=sp.stats.skew(i,axis=0,bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/1-Back/S01-01-25.09.2016.10.21.24.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(23552, 14)\n",
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/2-Back/S01-01-25.09.2016.10.27.55.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(22400, 14)\n",
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/Dual-1-Back/S01-01-25.09.2016.10.35.36.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(23808, 14)\n",
      "Extracting EDF parameters from /home/abhishek/Documents/MTP_HumanReliability/Mental_Workload/Cognitive-Mental-workload-Classification/Training-Data/S01/Dual-2-Back/S01-01-25.09.2016.10.45.01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(18048, 14)\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "idlefiles  = [f for f in listdir('Training-Data/S01/Idle') if isfile(join('Training-Data/S01/Idle', f))] \n",
    "OneBackfiles = [f for f in listdir('Training-Data/S01/1-Back') if isfile(join('Training-Data/S01/1-Back', f))]\n",
    "TwoBackfiles = [f for f in listdir('Training-Data/S01/2-Back') if isfile(join('Training-Data/S01/2-Back', f))]\n",
    "DualOneBackfiles = [f for f in listdir('Training-Data/S01/Dual-1-Back') if isfile(join('Training-Data/S01/Dual-1-Back', f))]\n",
    "DualTwoBackfiles = [f for f in listdir('Training-Data/S01/Dual-2-Back') if isfile(join('Training-Data/S01/Dual-2-Back', f))]\n",
    "\n",
    "files = []\n",
    "\n",
    "\n",
    "for i in idlefiles:\n",
    "    files.append([i,'Idle'])\n",
    "\n",
    "for i in OneBackfiles:\n",
    "    files.append([i,'1-Back'])\n",
    "\n",
    "for i in TwoBackfiles:\n",
    "   files.append([i,'2-Back'])\n",
    "\n",
    "for i in DualOneBackfiles:\n",
    "   files.append([i,'Dual-1-Back'])\n",
    "\n",
    "for i in DualTwoBackfiles:\n",
    "   files.append([i,'Dual-2-Back'])\n",
    "csvfile = 'Features/features.csv'\n",
    "mypath = 'Training-Data/S01/'\n",
    "# mypath = 'Training-Data/'\n",
    "\n",
    "with open(csvfile, \"a\",newline='') as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow(names)\n",
    "    for counter in range(len(files)):\n",
    "        subfolder =  files[counter][1]\n",
    "        # print(subfolder)\n",
    "        tag = files[counter][1] \n",
    "        # print(tag)\n",
    "\n",
    "        ############################# Changed part ########################################\n",
    "        data_path = mypath + subfolder +'/'+files[counter][0]\n",
    "        data = mne.io.read_raw_edf(data_path)\n",
    "        # print(data.get_data().shape)\n",
    "\n",
    "        \n",
    "        ###################################################################################\n",
    "        # rows = csv.reader(open(data_path))\n",
    "        # print(type(rows))\n",
    "        # sigbufs = [l for l in rows]\n",
    "        rows = data.get_data()\n",
    "        sigbufs = [l for l in rows]\n",
    "        sigbufs = np.array(sigbufs)\n",
    "        sigbufs = sigbufs.transpose()\n",
    "        sigbufs = sigbufs.astype(float)\n",
    "        sigbufs = sigbufs[:,3:17]\n",
    "        print('===================')\n",
    "        print(sigbufs.shape)\n",
    "        print('===================')\n",
    "\n",
    "        for i in np.arange(1,150,3):\n",
    "            features = []\n",
    "            epoch = sigbufs[i*128:(i+3)*128,:]\n",
    "            # print(len(epoch[0]))\n",
    "            # print('===================')\n",
    "            if (len(epoch)==0 or len(epoch[0]) == 0):\n",
    "                break\n",
    "        \n",
    "            #Coeffeicient of Variation\n",
    "            features.append(coeff_var(epoch))\n",
    "\n",
    "            #Mean of Vertex to Vertex Slope\n",
    "            features.append(slope_mean(epoch))\n",
    "            \n",
    "            #Variance of Vertex to Vertex Slope\n",
    "            features.append(slope_var(epoch))\n",
    "\n",
    "            #Hjorth Parameters\n",
    "            feature_list = hjorth(epoch)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "            \n",
    "            #Kurtosis\n",
    "            features.append(kurtosis(epoch))\n",
    "\n",
    "            #Second Difference Mean\n",
    "            features.append(secDiffMean(epoch))\n",
    "\n",
    "            #Second Difference Max\n",
    "            features.append(secDiffMax(epoch))\n",
    "               \n",
    "            #Skewness\n",
    "            features.append(skewness(epoch))\n",
    " \n",
    "            #First Difference Mean\n",
    "            features.append(first_diff_mean(epoch))\n",
    "\n",
    "            #First Difference Max\n",
    "            features.append(first_diff_max(epoch))\n",
    "\n",
    "            # print(type(epoch))\n",
    "            epoch_t = epoch.transpose()\n",
    "            #FFT Max Power - Delta, Theta, Alpha & Beta Band!\n",
    "            # feature_list  =  maxPwelch(epoch,128)            \n",
    "            feature_list  =  maxPwelch(epoch_t,128)            \n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "            #FFT Frequency Ratios\n",
    "            features.append(feature_list[0]/feature_list[1])\n",
    "            features.append(feature_list[0]/feature_list[2])\n",
    "            features.append(feature_list[1]/feature_list[3])\n",
    "            features.append((feature_list[0] + feature_list[1])/feature_list[2])\n",
    "            \n",
    "            #Wavelet Fetures!      lineterminator\n",
    "            feature_list = wavelet_features(epoch,14)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "                    \n",
    "            #Autoregressive model Coefficients\n",
    "            feature_list = autogressiveModelParametersBurg(epoch)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat.real)\n",
    "       \n",
    "            \"\"\"   e/S01-01-25.09.2016.10.52.46.edf\n",
    "<class '_csv.reader'>(epoch)idlefiles  = [f for f in listdir('Training-Data/S01/Idle') if isfile(join('Training-Data/S01/Idle', f))] \n",
    "            print('Shape SE = ',np.shape(abc))\n",
    "            features.append(abc)\n",
    "            \n",
    "            #Spectral Entropy\n",
    "            feature_list = spectral_entropy(epoch,128)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)        \n",
    "            \"\"\"\n",
    "            \n",
    "            if tag == 'Idle':\n",
    "                myClass = 1\n",
    "\n",
    "            if tag == '1-Back':\n",
    "                myClass = 2\n",
    "            \n",
    "            if tag == '2-Back':\n",
    "                myClass = 3\n",
    "\n",
    "            if tag == 'Dual-1-Back':\n",
    "                myClass = 4\n",
    "                \n",
    "            if tag == 'Dual-2-Back':\n",
    "                myClass = 5\n",
    "                \n",
    "            features.append(myClass);        \n",
    "            writer.writerow(features)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = csv.reader(open('Features/features.csv')) \n",
    "lines = [l for l in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Coeffiecient of Variation  Mean of Vertex to Vertex Slope  \\\n",
      "6.677861                   0.000138                    3.706754e-07   \n",
      "6.690996                   0.000021                    4.364432e-08   \n",
      "6.687605                   0.000107                    1.810834e-08   \n",
      "6.682289                   0.000024                    1.586161e-08   \n",
      "6.667006                   0.000030                    1.447848e-08   \n",
      "...                             ...                             ...   \n",
      "6.665415                   0.000033                    1.327888e-08   \n",
      "6.660998                   0.000051                    1.213679e-08   \n",
      "6.659469                   0.000010                    2.594040e-08   \n",
      "6.595089                   0.000025                    1.761993e-08   \n",
      "4.440761                   0.000059                    1.047006e-08   \n",
      "\n",
      "          Variance of Vertex to Vertex Slope  Hjorth_Activity  \\\n",
      "6.677861                            0.000025        28.377121   \n",
      "6.690996                            0.000025        28.327738   \n",
      "6.687605                            0.000025        28.362774   \n",
      "6.682289                            0.000025        28.363938   \n",
      "6.667006                            0.000025        28.374906   \n",
      "...                                      ...              ...   \n",
      "6.665415                            0.000025        28.378101   \n",
      "6.660998                            0.000025        28.323714   \n",
      "6.659469                            0.000025        28.381377   \n",
      "6.595089                            0.000025        28.353511   \n",
      "4.440761                            0.000017        18.912248   \n",
      "\n",
      "          Hjorth_Mobility  Hjorth_Complexity  Kurtosis  2nd Difference Mean  \\\n",
      "6.677861        27.773573         245.588225  0.009428             0.101009   \n",
      "6.690996        27.681860         248.520252  0.008971             0.101078   \n",
      "6.687605        27.482791         248.710995  0.008807             0.101196   \n",
      "6.682289        27.558418         248.801149  0.008749             0.101090   \n",
      "6.667006        27.552605         248.785994  0.008727             0.100995   \n",
      "...                   ...                ...       ...                  ...   \n",
      "6.665415        27.532753         248.819840  0.008710             0.101041   \n",
      "6.660998        27.504205         248.787971  0.008701             0.100768   \n",
      "6.659469        27.538786         248.659604  0.008717             0.100469   \n",
      "6.595089        27.557369         248.754131  0.008641             0.099906   \n",
      "4.440761        18.347842         165.863432  0.005814             0.067226   \n",
      "\n",
      "          2nd Difference Max  Skewness  ...      AR33      AR34      AR35  \\\n",
      "6.677861          -90.345580  0.008911  ... -0.993158 -0.027157  0.064213   \n",
      "6.690996          -91.176929  0.008527  ... -0.991169 -0.031869  0.067332   \n",
      "6.687605          -91.224572  0.008346  ... -1.000776 -0.014306  0.059466   \n",
      "6.682289          -91.246930  0.008290  ... -0.987697 -0.042103  0.074377   \n",
      "6.667006          -91.243222  0.008258  ... -1.008243 -0.034007  0.069310   \n",
      "...                      ...       ...  ...       ...       ...       ...   \n",
      "6.665415          -91.251501  0.008224  ... -1.004963 -0.043369  0.075291   \n",
      "6.660998          -91.243639  0.008188  ... -1.001726 -0.044068  0.078667   \n",
      "6.659469          -91.212398  0.008292  ... -1.025533  0.001076  0.053017   \n",
      "6.595089          -91.235452  0.008165  ... -0.989493 -0.077087  0.095910   \n",
      "4.440761          -60.830266  0.005496  ... -1.004941 -0.031687  0.065931   \n",
      "\n",
      "              AR36      AR37      AR38      AR39      AR40      AR41  AR42  \n",
      "6.677861 -1.003878 -0.007078  0.054739 -1.006370 -0.002898  0.053128     1  \n",
      "6.690996 -0.990189 -0.033936  0.068495 -1.003029 -0.013533  0.060815     1  \n",
      "6.687605 -1.016188  0.010946  0.049486 -0.996536 -0.019658  0.060659     1  \n",
      "6.682289 -1.010060 -0.026450  0.069039 -0.989562 -0.067783  0.084516     1  \n",
      "6.667006 -1.018425 -0.017285  0.064599 -0.993419 -0.059066  0.080595     1  \n",
      "...            ...       ...       ...       ...       ...       ...   ...  \n",
      "6.665415 -0.989712 -0.057961  0.080296 -1.008562 -0.033101  0.070152     5  \n",
      "6.660998 -1.021980 -0.017105  0.067838 -1.003659 -0.048138  0.081379     5  \n",
      "6.659469 -1.011866 -0.018786  0.060007 -1.013202 -0.015119  0.055195     5  \n",
      "6.595089 -1.005871 -0.054814  0.087664 -0.985306 -0.076808  0.094372     5  \n",
      "4.440761 -1.008104 -0.031617  0.066607 -0.990784 -0.048649  0.071872     5  \n",
      "\n",
      "[247 rows x 174 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Features/features.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7420/2681518468.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.mean(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AR42    2.975709\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7420/649938150.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.std(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AR42    1.406814\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(lines[1])-1):\n",
    "for i in range(len(lines[0])-1):  \n",
    "    columns = []\n",
    "    for j in range(1,len(lines)):\n",
    "        # print(i, j, lines[j][i])\n",
    "        columns.append(float(lines[j][i]))\n",
    "    # print(columns)\n",
    "    mean = np.mean(columns,axis = 0)\n",
    "    #print('\\nMean = ',mean)\n",
    "    std_dev  = np.std(columns,axis = 0)\n",
    "    #print('\\nSTD Deviation = ',std_dev)\n",
    "    for j in range(1,len(lines)):\n",
    "        lines[j][i] = (float(lines[j][i])-mean)/std_dev\n",
    "\n",
    "# print(lines)\n",
    "writer = csv.writer(open('Features/Normalizedfeatures.csv', 'w'))\n",
    "writer.writerows(lines)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
